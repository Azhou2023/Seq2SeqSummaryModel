{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import contraction\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, CuDNNLSTM, Embedding, Dense, Concatenate, TimeDistributed, LSTM\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from util import attention\n",
    "from matplotlib import pyplot\n",
    "import json\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "      for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "data = pd.read_csv(\"./Data/dblp-v10.csv\", nrows=75000)\n",
    "\n",
    "data.drop_duplicates(subset=[\"abstract\"], inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_28232\\3527155408.py:41: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['cleaned_summary'].replace('', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction.contraction_mapping[t] if t in contraction.contraction_mapping else t for t in newString.split(\" \")])\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "\n",
    "    cleaned_text = []\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:\n",
    "            cleaned_text.append(i)\n",
    "    return (\" \".join(cleaned_text)).strip()\n",
    "\n",
    "def clean_summary(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction.contraction_mapping[t] if t in contraction.contraction_mapping else t for t in newString.split(\" \")])\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = newString.split()\n",
    "\n",
    "    cleaned_text = []\n",
    "    for i in tokens:\n",
    "        if len(i)>1:\n",
    "            cleaned_text.append(i)\n",
    "    return \" \".join(cleaned_text)\n",
    "\n",
    "cleaned_text = []\n",
    "for i in data['abstract']:\n",
    "    cleaned_text.append(clean_text(i))\n",
    "cleaned_summary = []\n",
    "for i in data['title']:\n",
    "    cleaned_summary.append(clean_summary(i))\n",
    "\n",
    "data['cleaned_text'] = cleaned_text\n",
    "data['cleaned_summary'] = cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x: '_START_' + x + '_END_')\n",
    "\n",
    "max_len_text=200 \n",
    "max_len_summary=20\n",
    "latent_dim = 500\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(data['cleaned_text'], data['cleaned_summary'], test_size=0.1, random_state=0, shuffle=True);\n",
    "\n",
    "#Tokenizers\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "x_tr = x_tokenizer.texts_to_sequences(x_tr)\n",
    "x_tr = pad_sequences(x_tr, maxlen=max_len_text, padding='post')\n",
    "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
    "x_val = pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size = len(x_tokenizer.word_index)+1\n",
    "\n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "y_tr = y_tokenizer.texts_to_sequences(y_tr)\n",
    "y_tr = pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
    "y_val = y_tokenizer.texts_to_sequences(y_val)\n",
    "y_val = pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_voc_size = len(y_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K.clear_session()\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len_text))\n",
    "enc_emb = Embedding(x_voc_size, latent_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "encoder_lstm1 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "encoder_lstm2 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "encoder_lstm3 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(y_voc_size, latent_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# attn_out = AdditiveAttention()([encoder_outputs, decoder_outputs])\n",
    "\n",
    "attn_layer = attention.AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attn_out])\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=2, mode='auto', verbose=1)\n",
    "history = model.fit([x_tr, y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:,1:], epochs=50, callbacks=[es], batch_size=512, validation_data=([x_val, y_val[:,:-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:,1:]))\n",
    "model.save(\"./seq2seq.keras\")\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence\n",
      "Input: In this paper we study the nature of factors that facilitate mobile data services use, as well as the characteristics of early adopters, to shed light into diffusion patterns and inform predictions for future growth. We advocate that the use of mobile data services can be associated with one's level of satisfaction with his/her life. Based on the findings of a questionnaire-based survey (N=388), we have found that users satisfied with their personal life use information, mobile e-mail, and stock broking services more frequently than dissatisfied ones, while users satisfied with their professional life tend to use financial, information, and mobile e-mail services more heavily. Furthermore, we identify early adopters' profiles in terms of their demographic characteristics (gender, age, education, and income) to inform the design of effective target marketing strategies\n",
      "Predicted summary:  mobile data service for the study of mobile applications\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# print(np.asarray(dec_emb_layer.get_weights()).shape)\n",
    "\n",
    "\n",
    "\n",
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=x_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index\n",
    "\n",
    "def build_models(loaded_model):\n",
    "    encoder_inputs = Input(shape=(max_len_text))\n",
    "    embedding = Embedding(x_voc_size, latent_dim, trainable=True)\n",
    "    enc_emb = embedding(encoder_inputs)\n",
    "\n",
    "    encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "    encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "    encoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "\n",
    "    embedding.set_weights(loaded_model.layers[1].get_weights())\n",
    "    encoder_lstm1.set_weights(loaded_model.layers[2].get_weights())\n",
    "    encoder_lstm2.set_weights(loaded_model.layers[4].get_weights())\n",
    "    encoder_lstm3.set_weights(loaded_model.layers[6].get_weights())\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    dec_emb_layer = Embedding(y_voc_size, latent_dim, trainable=True)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "    attention_layer = attention.AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attention_layer([decoder_hidden_state_input, decoder_outputs]) \n",
    "\n",
    "    decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attn_out])\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "    dec_emb_layer.set_weights(loaded_model.layers[5].get_weights())\n",
    "    decoder_lstm.set_weights(loaded_model.layers[7].get_weights())\n",
    "    attention_layer.set_weights(loaded_model.layers[8].get_weights())\n",
    "    decoder_dense.set_weights(loaded_model.layers[10].get_weights())\n",
    "\n",
    "    encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "    decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c], [decoder_outputs] + [decoder_fwd_state, decoder_back_state])\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose=0)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "#Convert cudnnlstm layers to lstm layers to be CPU compatible\n",
    "loaded_model = tf.keras.models.load_model(\"seq2seq.keras\", custom_objects={\"AttentionLayer\": attention.AttentionLayer})\n",
    "json_config = loaded_model.to_json()\n",
    "json_config_= json.loads(json_config)\n",
    "layers = json_config_['config']['layers']\n",
    "for layer in layers:\n",
    "    if layer['class_name'].lower() == 'cudnnlstm':\n",
    "        layer['class_name'] = 'LSTM'\n",
    "model_json = json.dumps(json_config_)\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"seq2seq.keras\")\n",
    "encoder, decoder = build_models(model)\n",
    "\n",
    "print(\"Enter a sentence\")\n",
    "x = input()\n",
    "print(\"Input: \" + x)\n",
    "cleaned_text = [clean_text(x)]\n",
    "cleaned_text = x_tokenizer.texts_to_sequences(cleaned_text)\n",
    "cleaned_text = pad_sequences(cleaned_text, maxlen=max_len_text, padding='post')\n",
    "print(\"Predicted summary:\",decode_sequence(np.asarray(cleaned_text[0]).reshape(1,max_len_text), encoder, decoder))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
