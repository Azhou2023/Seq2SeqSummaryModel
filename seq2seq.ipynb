{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import contraction\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, CuDNNLSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from util import attention\n",
    "from matplotlib import pyplot\n",
    "import json\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "      for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "data = pd.read_csv(\"./Data/Reviews.csv\", nrows=100000)\n",
    "\n",
    "data.drop_duplicates(subset=[\"Text\"], inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction.contraction_mapping[t] if t in contraction.contraction_mapping else t for t in newString.split(\" \")])\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "\n",
    "    cleaned_text = []\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:\n",
    "            cleaned_text.append(i)\n",
    "    return (\" \".join(cleaned_text)).strip()\n",
    "\n",
    "def clean_summary(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction.contraction_mapping[t] if t in contraction.contraction_mapping else t for t in newString.split(\" \")])\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = newString.split()\n",
    "\n",
    "    cleaned_text = []\n",
    "    for i in tokens:\n",
    "        if len(i)>1:\n",
    "            cleaned_text.append(i)\n",
    "    return \" \".join(cleaned_text)\n",
    "\n",
    "cleaned_text = []\n",
    "for i in data['Text']:\n",
    "    cleaned_text.append(clean_text(i))\n",
    "cleaned_summary = []\n",
    "for i in data['Summary']:\n",
    "    cleaned_summary.append(clean_summary(i))\n",
    "\n",
    "data['cleaned_text'] = cleaned_text\n",
    "data['cleaned_summary'] = cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x: '_START_' + x + '_END_')\n",
    "\n",
    "max_len_text=80 \n",
    "max_len_summary=10\n",
    "latent_dim = 500\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(data['cleaned_text'], data['cleaned_summary'], test_size=0.1, random_state=0, shuffle=True);\n",
    "\n",
    "#Tokenizers\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "x_tr = x_tokenizer.texts_to_sequences(x_tr)\n",
    "x_tr = pad_sequences(x_tr, maxlen=max_len_text, padding='post')\n",
    "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
    "x_val = pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size = len(x_tokenizer.word_index)+1\n",
    "\n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "y_tr = y_tokenizer.texts_to_sequences(y_tr)\n",
    "y_tr = pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
    "y_val = y_tokenizer.texts_to_sequences(y_val)\n",
    "y_val = pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_voc_size = len(y_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K.clear_session()\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len_text))\n",
    "enc_emb = Embedding(x_voc_size, latent_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "encoder_lstm1 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "encoder_lstm2 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "encoder_lstm3 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(y_voc_size, latent_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# attn_out = AdditiveAttention()([encoder_outputs, decoder_outputs])\n",
    "\n",
    "attn_layer = attention.AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attn_out])\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=2, mode='auto', verbose=1)\n",
    "history = model.fit([x_tr, y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:,1:], epochs=50, callbacks=[es], batch_size=512, validation_data=([x_val, y_val[:,:-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:,1:]))\n",
    "model.save(\"./seq2seq.keras\")\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "not readable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m             layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    115\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(json_config_, outfile)\n\u001b[1;32m--> 116\u001b[0m     model_json \u001b[38;5;241m=\u001b[39m \u001b[43moutfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m model \u001b[38;5;241m=\u001b[39m model_from_json(model_json)\n\u001b[0;32m    118\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: not readable"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# print(np.asarray(dec_emb_layer.get_weights()).shape)\n",
    "\n",
    "\n",
    "\n",
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=x_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index\n",
    "\n",
    "def build_models(loaded_model):\n",
    "    encoder_inputs = Input(shape=(max_len_text))\n",
    "    embedding = Embedding(x_voc_size, latent_dim, trainable=True)\n",
    "    enc_emb = embedding(encoder_inputs)\n",
    "\n",
    "    encoder_lstm1 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "    encoder_lstm2 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "    encoder_lstm3 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "\n",
    "    embedding.set_weights(loaded_model.layers[1].get_weights())\n",
    "    encoder_lstm1.set_weights(loaded_model.layers[2].get_weights())\n",
    "    encoder_lstm2.set_weights(loaded_model.layers[4].get_weights())\n",
    "    encoder_lstm3.set_weights(loaded_model.layers[6].get_weights())\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    dec_emb_layer = Embedding(y_voc_size, latent_dim, trainable=True)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "    attention_layer = attention.AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attention_layer([decoder_hidden_state_input, decoder_outputs]) \n",
    "\n",
    "    decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attn_out])\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "    dec_emb_layer.set_weights(loaded_model.layers[5].get_weights())\n",
    "    decoder_lstm.set_weights(loaded_model.layers[7].get_weights())\n",
    "    attention_layer.set_weights(loaded_model.layers[8].get_weights())\n",
    "    decoder_dense.set_weights(loaded_model.layers[10].get_weights())\n",
    "\n",
    "    encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "    decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c], [decoder_outputs] + [decoder_fwd_state, decoder_back_state])\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose=0)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "        newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if(i!=0):\n",
    "        newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\"seq2seq.keras\", custom_objects={\"AttentionLayer\": attention.AttentionLayer})\n",
    "encoder, decoder = build_models(loaded_model)\n",
    "\n",
    "print(\"Enter a sentence\")\n",
    "x = input()\n",
    "print(\"Input: \" + x)\n",
    "cleaned_text = [clean_text(x)]\n",
    "cleaned_text = x_tokenizer.texts_to_sequences(cleaned_text)\n",
    "cleaned_text = pad_sequences(cleaned_text, maxlen=max_len_text, padding='post')\n",
    "print(\"Predicted summary:\",decode_sequence(np.asarray(cleaned_text[0]).reshape(1,max_len_text), encoder, decoder))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
